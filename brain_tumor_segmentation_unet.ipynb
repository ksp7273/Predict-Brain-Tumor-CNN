{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brain Tumor Segmentation with U-Net\n",
    "\n",
    "This notebook implements a U-Net model for brain tumor segmentation using MRI images. The dataset has images in `yes` (tumor) and `no` (non-tumor) subfolders, with corresponding masks.\n",
    "\n",
    "**Dependencies**: TensorFlow, NumPy, Matplotlib, OpenCV, Scikit-learn\n",
    "\n",
    "**Dataset**: MRI images in `data_dir/images/yes` and `data_dir/images/no`, with masks in `data_dir/masks`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Concatenate, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset\n",
    "\n",
    "Define paths to the dataset. Images are in `images/yes` and `images/no` subfolders, with corresponding masks in the `masks` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset paths (adjust based on your dataset structure)\n",
    "data_dir = 'path/to/dataset'  # Update to your dataset path\n",
    "image_dir = os.path.join(data_dir, 'images')\n",
    "yes_dir = os.path.join(image_dir, 'yes')\n",
    "no_dir = os.path.join(image_dir, 'no')\n",
    "mask_dir = os.path.join(data_dir, 'masks')\n",
    "\n",
    "# Image parameters\n",
    "IMG_HEIGHT = 128\n",
    "IMG_WIDTH = 128\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis (EDA)\n",
    "\n",
    "Analyze the dataset to understand its structure, including the number of images in `yes` and `no` folders and masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of image and mask files\n",
    "yes_files = [os.path.join(yes_dir, f) for f in os.listdir(yes_dir) if f.endswith(('.jpg', '.png'))]\n",
    "no_files = [os.path.join(no_dir, f) for f in os.listdir(no_dir) if f.endswith(('.jpg', '.png'))]\n",
    "image_files = yes_files + no_files\n",
    "mask_files = [os.path.join(mask_dir, f) for f in os.listdir(mask_dir) if f.endswith(('.jpg', '.png'))]\n",
    "\n",
    "# Check dataset size and consistency\n",
    "print(f'Number of tumor images (yes): {len(yes_files)}')\n",
    "print(f'Number of non-tumor images (no): {len(no_files)}')\n",
    "print(f'Total images: {len(image_files)}')\n",
    "print(f'Number of masks: {len(mask_files)}')\n",
    "assert len(image_files) == len(mask_files), 'Number of images and masks must match'\n",
    "\n",
    "# Check sample image and mask dimensions\n",
    "sample_img = cv2.imread(image_files[0])\n",
    "sample_mask = cv2.imread(mask_files[0], cv2.IMREAD_GRAYSCALE)\n",
    "print(f'Sample image shape: {sample_img.shape}')\n",
    "print(f'Sample mask shape: {sample_mask.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Plot\n",
    "\n",
    "Visualize sample images and their corresponding masks to understand the data distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot sample images and masks\n",
    "plt.figure(figsize=(10, 5))\n",
    "for i in range(3):\n",
    "    img = cv2.imread(image_files[i])\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    mask = cv2.imread(mask_files[i], cv2.IMREAD_GRAYSCALE)\n",
    "    \n",
    "    plt.subplot(2, 3, i+1)\n",
    "    plt.imshow(img)\n",
    "    plt.title('MRI Image')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(2, 3, i+4)\n",
    "    plt.imshow(mask, cmap='gray')\n",
    "    plt.title('Mask')\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Augmentation\n",
    "\n",
    "Apply data augmentation to training images and masks to increase dataset diversity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_generator(img_files, mask_files, target_size, batch_size, augment=False):\n",
    "    if augment:\n",
    "        img_datagen = ImageDataGenerator(\n",
    "            rescale=1./255,\n",
    "            rotation_range=20,\n",
    "            width_shift_range=0.2,\n",
    "            height_shift_range=0.2,\n",
    "            shear_range=0.2,\n",
    "            zoom_range=0.2,\n",
    "            horizontal_flip=True,\n",
    "            fill_mode='nearest'\n",
    "        )\n",
    "        mask_datagen = ImageDataGenerator(\n",
    "            rescale=1./255,\n",
    "            rotation_range=20,\n",
    "            width_shift_range=0.2,\n",
    "            height_shift_range=0.2,\n",
    "            shear_range=0.2,\n",
    "            zoom_range=0.2,\n",
    "            horizontal_flip=True,\n",
    "            fill_mode='nearest'\n",
    "        )\n",
    "    else:\n",
    "        img_datagen = ImageDataGenerator(rescale=1./255)\n",
    "        mask_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "    # Create generators for images and masks\n",
    "    def flow_from_files(datagen, files, target_size, batch_size, color_mode, shuffle=True, seed=42):\n",
    "        while True:\n",
    "            indices = np.arange(len(files))\n",
    "            if shuffle:\n",
    "                np.random.seed(seed)\n",
    "                np.random.shuffle(indices)\n",
    "            for i in range(0, len(files), batch_size):\n",
    "                batch_indices = indices[i:i + batch_size]\n",
    "                batch_files = [files[idx] for idx in batch_indices]\n",
    "                batch_images = [cv2.imread(f, cv2.IMREAD_COLOR if color_mode == 'rgb' else cv2.IMREAD_GRAYSCALE) for f in batch_files]\n",
    "                batch_images = [cv2.resize(img, target_size) for img in batch_images]\n",
    "                batch_images = np.array(batch_images) / 255.0\n",
    "                yield batch_images\n",
    "\n",
    "    img_generator = flow_from_files(img_datagen, img_files, target_size, batch_size, color_mode='rgb')\n",
    "    mask_generator = flow_from_files(mask_datagen, mask_files, target_size, batch_size, color_mode='grayscale')\n",
    "    return zip(img_generator, mask_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Preprocessing\n",
    "\n",
    "Apply preprocessing steps: convert BGR to grayscale, apply GaussianBlur, threshold, erode, dilate, and find contours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(img_path, target_size=(IMG_HEIGHT, IMG_WIDTH)):\n",
    "    img = cv2.imread(img_path)\n",
    "    # Convert BGR to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    # Apply GaussianBlur\n",
    "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "    # Apply thresholding\n",
    "    _, thresh = cv2.threshold(blurred, 127, 255, cv2.THRESH_BINARY)\n",
    "    # Erode and dilate\n",
    "    kernel = np.ones((3, 3), np.uint8)\n",
    "    eroded = cv2.erode(thresh, kernel, iterations=1)\n",
    "    dilated = cv2.dilate(eroded, kernel, iterations=1)\n",
    "    # Find contours\n",
    "    contours, _ = cv2.findContours(dilated, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    # Draw contours on original image (optional, for visualization)\n",
    "    contour_img = img.copy()\n",
    "    cv2.drawContours(contour_img, contours, -1, (0, 255, 0), 2)\n",
    "    # Resize to target size\n",
    "    processed_img = cv2.resize(dilated, target_size)\n",
    "    # Convert back to 3 channels for model input\n",
    "    processed_img = cv2.cvtColor(processed_img, cv2.COLOR_GRAY2RGB)\n",
    "    processed_img = processed_img / 255.0\n",
    "    return processed_img\n",
    "\n",
    "def preprocess_mask(mask_path, target_size=(IMG_HEIGHT, IMG_WIDTH)):\n",
    "    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "    # Apply thresholding to ensure binary mask\n",
    "    _, mask = cv2.threshold(mask, 127, 255, cv2.THRESH_BINARY)\n",
    "    # Resize to target size\n",
    "    mask = cv2.resize(mask, target_size)\n",
    "    mask = mask / 255.0\n",
    "    return mask[..., np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Image Loading\n",
    "\n",
    "Load and preprocess images and masks from `yes`, `no`, and `masks` folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images and masks\n",
    "images = np.array([preprocess_image(f) for f in image_files])\n",
    "masks = np.array([preprocess_mask(f) for f in mask_files])\n",
    "\n",
    "print(f'Loaded images shape: {images.shape}')\n",
    "print(f'Loaded masks shape: {masks.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Data Splitting (Train, Test, Validation)\n",
    "\n",
    "Split the dataset into training, validation, and test sets (70% train, 15% validation, 15% test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(images, masks, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f'Training set: {X_train.shape[0]} images')\n",
    "print(f'Validation set: {X_val.shape[0]} images')\n",
    "print(f'Test set: {X_test.shape[0]} images')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. U-Net Model Building and Training\n",
    "\n",
    "Define and train the U-Net model for segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet_model(input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)):\n",
    "    inputs = Input(input_shape)\n",
    "    \n",
    "    # Encoder\n",
    "    c1 = Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    c1 = Conv2D(64, (3, 3), activation='relu', padding='same')(c1)\n",
    "    p1 = MaxPooling2D((2, 2))(c1)\n",
    "    \n",
    "    c2 = Conv2D(128, (3, 3), activation='relu', padding='same')(p1)\n",
    "    c2 = Conv2D(128, (3, 3), activation='relu', padding='same')(c2)\n",
    "    p2 = MaxPooling2D((2, 2))(c2)\n",
    "    \n",
    "    c3 = Conv2D(256, (3, 3), activation='relu', padding='same')(p2)\n",
    "    c3 = Conv2D(256, (3, 3), activation='relu', padding='same')(c3)\n",
    "    p3 = MaxPooling2D((2, 2))(c3)\n",
    "    \n",
    "    c4 = Conv2D(512, (3, 3), activation='relu', padding='same')(p3)\n",
    "    c4 = Conv2D(512, (3, 3), activation='relu', padding='same')(c4)\n",
    "    p4 = MaxPooling2D((2, 2))(c4)\n",
    "    \n",
    "    # Bottleneck\n",
    "    c5 = Conv2D(1024, (3, 3), activation='relu', padding='same')(p4)\n",
    "    c5 = Conv2D(1024, (3, 3), activation='relu', padding='same')(c5)\n",
    "    \n",
    "    # Decoder\n",
    "    u6 = UpSampling2D((2, 2))(c5)\n",
    "    u6 = Concatenate()([u6, c4])\n",
    "    c6 = Conv2D(512, (3, 3), activation='relu', padding='same')(u6)\n",
    "    c6 = Conv2D(512, (3, 3), activation='relu', padding='same')(c6)\n",
    "    \n",
    "    u7 = UpSampling2D((2, 2))(c6)\n",
    "    u7 = Concatenate()([u7, c3])\n",
    "    c7 = Conv2D(256, (3, 3), activation='relu', padding='same')(u7)\n",
    "    c7 = Conv2D(256, (3, 3), activation='relu', padding='same')(c7)\n",
    "    \n",
    "    u8 = UpSampling2D((2, 2))(c7)\n",
    "    u8 = Concatenate()([u8, c2])\n",
    "    c8 = Conv2D(128, (3, 3), activation='relu', padding='same')(u8)\n",
    "    c8 = Conv2D(128, (3, 3), activation='relu', padding='same')(c8)\n",
    "    \n",
    "    u9 = UpSampling2D((2, 2))(c8)\n",
    "    u9 = Concatenate()([u9, c1])\n",
    "    c9 = Conv2D(64, (3, 3), activation='relu', padding='same')(u9)\n",
    "    c9 = Conv2D(64, (3, 3), activation='relu', padding='same')(c9)\n",
    "    \n",
    "    outputs = Conv2D(1, (1, 1), activation='sigmoid')(c9)\n",
    "    \n",
    "    return Model(inputs, outputs)\n",
    "\n",
    "# Create and compile model\n",
    "model = unet_model()\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Model summary\n",
    "model.summary()\n",
    "\n",
    "# Train model\n",
    "EPOCHS = 20\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(X_val, y_val)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Evaluation and Visualization\n",
    "\n",
    "Evaluate the model on the test set and visualize training/validation metrics and sample predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n",
    "\n",
    "# Save model\n",
    "model.save('brain_tumor_unet_model.h5')\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Visualize sample predictions\n",
    "predictions = model.predict(X_test[:3])\n",
    "plt.figure(figsize=(10, 5))\n",
    "for i in range(3):\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    plt.imshow(X_test[i])\n",
    "    plt.title('Input Image')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(3, 3, i+4)\n",
    "    plt.imshow(y_test[i].squeeze(), cmap='gray')\n",
    "    plt.title('Ground Truth')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(3, 3, i+7)\n",
    "    plt.imshow(predictions[i].squeeze(), cmap='gray')\n",
    "    plt.title('Prediction')\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
